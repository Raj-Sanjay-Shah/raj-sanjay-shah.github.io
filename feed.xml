<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://raj-sanjay-shah.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://raj-sanjay-shah.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-02-20T21:18:52+00:00</updated><id>https://raj-sanjay-shah.github.io/feed.xml</id><title type="html">blank</title><subtitle>PhD student in Interactive Computing at Georgia Tech. </subtitle><entry><title type="html">Domain-Specific Evaluations With Real Consequences</title><link href="https://raj-sanjay-shah.github.io/blog/2026/domain-evaluations/" rel="alternate" type="text/html" title="Domain-Specific Evaluations With Real Consequences"/><published>2026-02-20T13:00:00+00:00</published><updated>2026-02-20T13:00:00+00:00</updated><id>https://raj-sanjay-shah.github.io/blog/2026/domain-evaluations</id><content type="html" xml:base="https://raj-sanjay-shah.github.io/blog/2026/domain-evaluations/"><![CDATA[<p>Generic leaderboards rarely tell us whether an AI system actually helps an analyst detect fraud, support clinical decision-making, or flag cyber threats. A line through my work focuses on targeted evaluations that mirror those real-world stakes.</p> <h2 id="finance--medicine">Finance &amp; medicine</h2> <ul> <li><strong>When Flue Meets Flang (EMNLP 2022)</strong> released a domain-tuned benchmark blending filings, earnings calls, and analyst commentary, with metrics for numerical reasoning and entity resolution.</li> <li>Ongoing work expands this suite to medical text, evaluating reliability, explainability, and bias when models make patient-facing statements.</li> </ul> <h2 id="cyber-threat-intelligence">Cyber threat intelligence</h2> <ul> <li><strong>CTI-Twitter</strong> fused supervised + unsupervised signals so analysts could triage millions of tweets down to credible threat mentions.</li> <li>The pipeline now evaluates timeliness, trust, and downstream analyst handoff quality so practitioners can see where automation helps and where humans should intervene.</li> </ul> <h2 id="visualization-literacy-for-multimodal-models">Visualization literacy for multimodal models</h2> <ul> <li>Our NeurIPS 2024 work probed how zero-shot vision-language models recover graphical perception results.</li> <li>We built a visual-question battery covering trend detection, correlation misreads, and scale illusions, and compared model vs. human error profiles.</li> </ul> <h2 id="why-bespoke-evaluations-matter">Why bespoke evaluations matter</h2> <ul> <li><strong>Ground truth is contextual:</strong> the ‚Äúright‚Äù answer for a finance analyst is different than for a counselor or policy designer.</li> <li><strong>Failure costs vary:</strong> a hallucinated earnings number can move markets; a misread visualization can mislead thousands.</li> <li><strong>Trust grows with relevance:</strong> analysts cite benchmarks because they reflect their KPIs.</li> </ul> <h2 id="whats-next">What‚Äôs next</h2> <ul> <li>Scenario-based stress tests (‚ÄúCPI spikes 200 bps‚Äù) and chain-of-thought rubrics.</li> <li>Shared tasks where VLMs explain visualization mistakes, not just point them out.</li> <li>Pair cybersecurity datasets with the unlearning framework so sensitive indicators can be removed without breaking downstream analytics.</li> </ul> <p>If you have a domain with unusual evaluation needs, this playbook can encode your constraints into reproducible benchmarks.</p>]]></content><author><name></name></author><category term="research"/><category term="evaluation"/><category term="evaluation"/><category term="finance"/><category term="cybersecurity"/><category term="visualization"/><summary type="html"><![CDATA[Benchmarks for finance, medicine, and cybersecurity that mirror the consequences that matter.]]></summary></entry><entry><title type="html">Teaching Language Models to Grow Up</title><link href="https://raj-sanjay-shah.github.io/blog/2026/cognitive-alignment/" rel="alternate" type="text/html" title="Teaching Language Models to Grow Up"/><published>2026-02-20T12:00:00+00:00</published><updated>2026-02-20T12:00:00+00:00</updated><id>https://raj-sanjay-shah.github.io/blog/2026/cognitive-alignment</id><content type="html" xml:base="https://raj-sanjay-shah.github.io/blog/2026/cognitive-alignment/"><![CDATA[<p>Large language models can mimic adult conversation, but do they develop cognitive abilities the way humans do? Our thread-from <strong>Development of Cognitive Intelligence in Pre-trained Language Models (EMNLP 2024)</strong> to BabyLM workshops and typicality/numerical studies-treats LLMs as computational participants in cognitive science experiments.</p> <h2 id="why-developmental-alignment-matters">Why developmental alignment matters</h2> <ul> <li>Safety issues often stem from gaps between how models generalize and how humans learn concepts over time.</li> <li>Most benchmarks exercise mature competencies; we need probes that mirror human developmental progression.</li> <li>When models track human trajectories, we can design curricula that steer them earlier.</li> </ul> <h2 id="what-weve-built">What we‚Äôve built</h2> <ol> <li><strong>Developmentally staged corpora</strong> (BabyLM challenge) that restrict training to age-aligned text so abstractions emerge gradually.</li> <li><strong>Cognitive batteries</strong> testing garden-path recovery, numerical magnitude, and typicality, paired with linking hypotheses that map behavior to model attention.</li> <li><strong>Theory-practice loop</strong> where cognitive experiments reveal over-reliance on statistics, guiding safer training objectives.</li> </ol> <h2 id="takeaways">Takeaways</h2> <ul> <li>Smaller PLMs trained on child-directed data can hit many milestones but still mis-handle compositional syntax without curated scaffolding.</li> <li>Typicality and magnitude effects emerge in mid-sized models once prompts match human tasks.</li> <li>Garden-path fast recovery hints at over-reliance on surface statistics rather than incremental parsing.</li> </ul> <p>Next steps: public developmental leaderboards, curriculum interventions, and cross-disciplinary collaborations. Join the next BabyLM workshop and help turn ‚Äúteaching models to grow up‚Äù into an empirical science.</p>]]></content><author><name></name></author><category term="research"/><category term="cognition"/><category term="cognition"/><category term="babylm"/><category term="developmental-alignment"/><category term="llm"/><summary type="html"><![CDATA[BabyLM and psych-inspired batteries that treat LLMs as computational subjects of cognitive science.]]></summary></entry><entry><title type="html">Beyond the Unlearning Mirage</title><link href="https://raj-sanjay-shah.github.io/blog/2026/unlearning-safety/" rel="alternate" type="text/html" title="Beyond the Unlearning Mirage"/><published>2026-02-20T11:00:00+00:00</published><updated>2026-02-20T11:00:00+00:00</updated><id>https://raj-sanjay-shah.github.io/blog/2026/unlearning-safety</id><content type="html" xml:base="https://raj-sanjay-shah.github.io/blog/2026/unlearning-safety/"><![CDATA[<p>Most LLM unlearning demos crumble when users rephrase queries, add alias chains, or build multi-hop reasoning paths. ‚ÄúThe Unlearning Mirage‚Äù makes that brittleness visible by pairing dynamic, model-specific probes with activation-pathway analysis.</p> <h2 id="what-the-framework-delivers">What the framework delivers</h2> <ol> <li><strong>Probe generation</strong> that spans single-hop recalls to adversarial multi-hop chains and aliases, all grounded in knowledge elicited from the target model before unlearning.</li> <li><strong>Activation analysis</strong> showing that single-hop queries usually travel dominant pathways that the unlearning method disrupts, while multi-hop queries find alternate routes that often remain intact.</li> <li><strong>Automatic alias/paraphrase sweeps</strong> that stress-test whether a supposedly forgotten fact still leaks.</li> </ol> <h2 id="complementary-defenses">Complementary defenses</h2> <ul> <li><strong>Text watermarking taxonomy (NAACL 2025)</strong> clarifies how to prove which model family produced a generation, even after editing or fine-tuning.</li> <li><strong>Power-law continual learning (ECAI 2024)</strong> keeps models plastic while mitigating catastrophic forgetting, offering a softer alternative than hard erasure.</li> </ul> <h2 id="what-practitioners-get">What practitioners get</h2> <ul> <li>Pip-installable evaluators and leaderboards so every unlearning method can be stress-tested before deployment.</li> <li>Roadmaps for pairing unlearning with provenance controls and continual learning recipes.</li> <li>Visualizations that expose exactly which prompts still succeed post-unlearning.</li> </ul> <p>Next steps: release activation-visualization tooling, extend to multimodal models, and maintain a living playbook of best practices. If you operate production LLMs, this stack is built for you.</p>]]></content><author><name></name></author><category term="research"/><category term="safety"/><category term="safety"/><category term="unlearning"/><category term="watermarking"/><category term="continual-learning"/><summary type="html"><![CDATA[Dynamic probes and activation analysis that expose brittle unlearning, paired with watermarking and continual learning.]]></summary></entry><entry><title type="html">LLM Copilots for Peer Counselors</title><link href="https://raj-sanjay-shah.github.io/blog/2026/llm-peer-counselors/" rel="alternate" type="text/html" title="LLM Copilots for Peer Counselors"/><published>2026-02-20T10:00:00+00:00</published><updated>2026-02-20T10:00:00+00:00</updated><id>https://raj-sanjay-shah.github.io/blog/2026/llm-peer-counselors</id><content type="html" xml:base="https://raj-sanjay-shah.github.io/blog/2026/llm-peer-counselors/"><![CDATA[<p>Peer-support platforms run on empathy, reflective listening, and quick feedback, yet volunteers rarely get the same coaching professionals do. Over the past few years we built a full stack of AI copilots for counselors, culminating in <strong>Helping the Helper (CSCW 2025)</strong>, <strong>Modeling Motivational Interviewing Strategies (CSCW 2022)</strong>, <strong>Multi-level Feedback Generation (ACL 2024)</strong>, and the randomized trial <strong>Can LLM-Simulated Practice and Feedback Upskill Human Counselors? (2025)</strong>.</p> <h2 id="why-counselors-need-copilots">Why counselors need copilots</h2> <ul> <li>Volunteers scale faster than expert supervision and rarely receive structured practice.</li> <li>Feedback in these communities is episodic and subjective, so it‚Äôs hard to tell whether a reflection or affirmation helped.</li> <li>The highest-stakes conversations demand consistent quality, yet most platforms can‚Äôt afford 1:1 coaching.</li> </ul> <h2 id="what-we-built">What we built</h2> <ol> <li><strong>Skill-aware pipelines</strong> that encode motivational interviewing (MI) strategies and conversational intents across counselor-client turns.</li> <li><strong>LLM-based practice sandboxes</strong> where novices rehearse with simulated seekers, receive rubric-aligned feedback, and iterate within minutes.</li> <li><strong>Tiered feedback generators</strong> that surface tactical guidance (‚ÄúTry a double-sided reflection‚Äù) grounded in the volunteer‚Äôs actual transcripts.</li> <li><strong>Analytics for moderators</strong> that flag stuck counselors and suggest targeted interventions.</li> </ol> <h2 id="evidence-it-works">Evidence it works</h2> <ul> <li>Lab studies show statistically significant gains on MI competence scores after a single LLM practice session.</li> <li>Field deployments on large peer-support platforms reduced time-to-proficiency by ~30% while raising user-reported helpfulness.</li> <li>Counselors trust the assistant more when it mirrors MI language and cites concrete turns from their own conversations.</li> </ul> <h2 id="whats-next">What‚Äôs next</h2> <ul> <li>Adaptive curricula that match practice difficulty to longitudinal counselor data.</li> <li>Safety overlays that combine the unlearning + watermarking stack so sensitive disclosures stay in the secure perimeter.</li> <li>An open evaluation suite (datasets, rubrics, code) so other teams can benchmark their own coaching tools.</li> </ul> <p>If you want to experiment with the coaching sandbox or contribute new MI annotations, reach out-this line of work thrives on collaboration.</p>]]></content><author><name></name></author><category term="research"/><category term="mental-health"/><category term="mental-health"/><category term="counseling"/><category term="llm"/><category term="feedback"/><summary type="html"><![CDATA[How motivational interviewing‚Äìaware sandboxes and analytics help peer counselors level up.]]></summary></entry><entry><title type="html">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</title><link href="https://raj-sanjay-shah.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/" rel="alternate" type="text/html" title="Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra"/><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://raj-sanjay-shah.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra</id><content type="html" xml:base="https://raj-sanjay-shah.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/"><![CDATA[<p>Learn more:Learn more:Learn more:Learn more:Learn more:Learn more:May 14, 2024 We‚Äôre introducing a series of updates across the Gemini family of models, including the new 1.5 Flash, our lightweight model for speed and efficiency, and Project Astra, our vision for the future of AI assistants. In December, we launched our first natively multimodal model Gemini 1.0 in three sizes: Ultra, Pro and Nano. Just a few months later we released 1.5 Pro, with enhanced performance and a breakthrough long context window of 1 million tokens.Developers and enterprise customers have been putting 1.5 Pro to use in incredible ways and finding its long context window, multimodal reasoning capabilities and impressive overall performance incredibly useful.We know from user feedback that some applications need lower latency and a lower cost to serve. This inspired us to keep innovating, so today, we‚Äôre introducing Gemini 1.5 Flash: a model that‚Äôs lighter-weight than 1.5 Pro, and designed to be fast and efficient to serve at scale.Both 1.5 Pro and 1.5 Flash are available in public preview with a 1 million token context window in Google AI Studio and Vertex AI. And now, 1.5 Pro is also available with a 2 million token context window via waitlist to developers using the API and to Google Cloud customers.We‚Äôre also introducing updates across the Gemini family of models, announcing our next generation of open models, Gemma 2, and sharing progress on the future of AI assistants, with Project Astra.Context lengths of leading foundation models compared with Gemini 1.5‚Äôs 2 million token capability1.5 Flash is the newest addition to the Gemini model family and the fastest Gemini model served in the API. It‚Äôs optimized for high-volume, high-frequency tasks at scale, is more cost-efficient to serve and features our breakthrough long context window.While it‚Äôs a lighter weight model than 1.5 Pro, it‚Äôs highly capable of multimodal reasoning across vast amounts of information and delivers impressive quality for its size.The new Gemini 1.5 Flash model is optimized for speed and efficiency, is highly capable of multimodal reasoning and features our breakthrough long context window.1.5 Flash excels at summarization, chat applications, image and video captioning, data extraction from long documents and tables, and more. This is because it‚Äôs been trained by 1.5 Pro through a process called ‚Äúdistillation,‚Äù where the most essential knowledge and skills from a larger model are transferred to a smaller, more efficient model.Read more about 1.5 Flash in our updated Gemini 1.5 technical report, on the Gemini technology page, and learn about 1.5 Flash‚Äôs availability and pricing.Over the last few months, we‚Äôve significantly improved 1.5 Pro, our best model for general performance across a wide range of tasks.Beyond extending its context window to 2 million tokens, we‚Äôve enhanced its code generation, logical reasoning and planning, multi-turn conversation, and audio and image understanding through data and algorithmic advances. We see strong improvements on public and internal benchmarks for each of these tasks.1.5 Pro can now follow increasingly complex and nuanced instructions, including ones that specify product-level behavior involving role, format and style. We‚Äôve improved control over the model‚Äôs responses for specific use cases, like crafting the persona and response style of a chat agent or automating workflows through multiple function calls. And we‚Äôve enabled users to steer model behavior by setting system instructions.We added audio understanding in the Gemini API and Google AI Studio, so 1.5 Pro can now reason across image and audio for videos uploaded in Google AI Studio. And we‚Äôre now integrating 1.5 Pro into Google products, including Gemini Advanced and in Workspace apps.Read more about 1.5 Pro in our updated Gemini 1.5 technical report and on the Gemini technology page.Gemini Nano is expanding beyond text-only inputs to include images as well. Starting with Pixel, applications using Gemini Nano with Multimodality will be able to understand the world the way people do ‚Äî not just through text, but also through sight, sound and spoken language.Read more about Gemini 1.0 Nano on Android.Today, we‚Äôre also sharing a series of updates to Gemma, our family of open models built from the same research and technology used to create the Gemini models.We‚Äôre announcing Gemma 2, our next generation of open models for responsible AI innovation. Gemma 2 has a new architecture designed for breakthrough performance and efficiency, and will be available in new sizes.The Gemma family is also expanding with PaliGemma, our first vision-language model inspired by PaLI-3. And we‚Äôve upgraded our Responsible Generative AI Toolkit with LLM Comparator for evaluating the quality of model responses.Read more on the Developer blog.As part of Google DeepMind‚Äôs mission to build AI responsibly to benefit humanity, we‚Äôve always wanted to develop universal AI agents that can be helpful in everyday life. That‚Äôs why today, we‚Äôre sharing our progress in building the future of AI assistants with Project Astra (advanced seeing and talking responsive agent).To be truly useful, an agent needs to understand and respond to the complex and dynamic world just like people do ‚Äî and take in and remember what it sees and hears to understand context and take action. It also needs to be proactive, teachable and personal, so users can talk to it naturally and without lag or delay.While we‚Äôve made incredible progress developing AI systems that can understand multimodal information, getting response time down to something conversational is a difficult engineering challenge. Over the past few years, we‚Äôve been working to improve how our models perceive, reason and converse to make the pace and quality of interaction feel more natural.Building on Gemini, we‚Äôve developed prototype agents that can process information faster by continuously encoding video frames, combining the video and speech input into a timeline of events, and caching this information for efficient recall.By leveraging our leading speech models, we also enhanced how they sound, giving the agents a wider range of intonations. These agents can better understand the context they‚Äôre being used in, and respond quickly, in conversation.With technology like this, it‚Äôs easy to envision a future where people could have an expert AI assistant by their side, through a phone or glasses. And some of these capabilities are coming to Google products, like the Gemini app and web experience, later this year.We‚Äôve made incredible progress so far with our family of Gemini models, and we‚Äôre always striving to advance the state-of-the-art even further. By investing in a relentless production line of innovation, we‚Äôre able to explore new ideas at the frontier, while also unlocking the possibility of new and exciting Gemini use cases.Learn more about Gemini and its capabilities. Your information will be used in accordance with Google‚Äôs privacy policy.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>      Done. Just one step more.
    
      Check your inbox to confirm your subscription.
    You are already subscribed to our newsletter.
    You can also subscribe with a
    different email address
    
    .
    
  Let‚Äôs stay in touch. Get the latest news from Google in your inbox.
          Follow Us
</code></pre></div></div>]]></content><author><name></name></author><category term="external-posts"/><category term="google"/><summary type="html"><![CDATA[We‚Äôre sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.]]></summary></entry><entry><title type="html">Displaying External Posts on Your al-folio Blog</title><link href="https://raj-sanjay-shah.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/" rel="alternate" type="text/html" title="Displaying External Posts on Your al-folio Blog"/><published>2022-04-23T23:20:09+00:00</published><updated>2022-04-23T23:20:09+00:00</updated><id>https://raj-sanjay-shah.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog</id><content type="html" xml:base="https://raj-sanjay-shah.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/"><![CDATA[<h3>External Posts on Your al-folio¬†Blog</h3> <p>If you prefer publishing blog posts on medium.com or other external sources, starting version v0.5.0, <a href="https://github.com/alshedivat/al-folio">al-folio</a> lets you to display your external posts in the blog feed of your website!¬†üéâüéâ</p> <p>Configuring external sources of super simple. After upgrading to v0.5.0, just add the following section to your _config.yml:</p> <pre>external_sources:<br />  - name: medium.com  # name of the source (arbitrary string)<br />    rss_url: <a href="https://medium.com/@al-folio/feed">https://medium.com/@&lt;your-medium-username&gt;/feed</a></pre> <p>The example above adds your medium.com blog post feed as an external source. But you can add arbitrary RSS feeds as¬†sources.</p> <p>Any questions or suggestions? üëâ Start <a href="https://github.com/alshedivat/al-folio/discussions">a discussion on¬†GitHub</a>!</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=b60a1d241a0a" width="1" height="1" alt=""/></p>]]></content><author><name></name></author><category term="external-posts"/><category term="medium"/></entry></feed>